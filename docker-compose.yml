version: '3.8'

# ============================================
# Readtube Docker Compose Configuration
# ============================================

services:
  # Main Readtube service
  readtube:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: readtube
    volumes:
      - ./output:/app/output
      - ./cache:/app/cache
      - ./.transcript_cache:/app/.transcript_cache
    environment:
      - PYTHONUNBUFFERED=1
      - OUTPUT_DIR=/app/output
      - CACHE_DIR=/app/cache
    restart: unless-stopped
    # Override command for specific tasks (examples below)
    # command: ["fetch_transcript.py", "https://youtube.com/watch?v=VIDEO_ID"]

  # Ollama service for local LLM (optional)
  ollama:
    image: ollama/ollama:latest
    container_name: readtube-ollama
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    restart: unless-stopped
    profiles:
      - llm
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # Readtube with Ollama integration
  readtube-with-ollama:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: readtube-ollama-client
    volumes:
      - ./output:/app/output
      - ./cache:/app/cache
    environment:
      - PYTHONUNBUFFERED=1
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=llama3.2
    depends_on:
      - ollama
    profiles:
      - llm
    restart: unless-stopped

volumes:
  ollama_data:

# ============================================
# Usage Examples
# ============================================
#
# Basic usage (fetch transcript):
#   docker compose run --rm readtube fetch_transcript.py "https://youtube.com/watch?v=VIDEO_ID"
#
# Fetch and save to JSON:
#   docker compose run --rm readtube fetch_transcript.py "https://youtube.com/watch?v=VIDEO_ID" --output-json /app/output/video.json
#
# Start with Ollama for local LLM:
#   docker compose --profile llm up -d ollama
#   # Wait for Ollama to start, then pull a model:
#   docker compose exec ollama ollama pull llama3.2
#   # Generate article:
#   docker compose --profile llm run --rm readtube-with-ollama write_article.py /app/output/video.json
#
# With Claude API:
#   ANTHROPIC_API_KEY=your-key docker compose run --rm \
#     -e ANTHROPIC_API_KEY readtube write_article.py /app/output/video.json
#
# Batch processing:
#   docker compose run --rm -v $(pwd)/config.yaml:/app/config.yaml:ro \
#     readtube batch.py /app/config.yaml
